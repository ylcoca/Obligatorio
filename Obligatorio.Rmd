---
title: "Obligatorio"
output: html_document
---

## Preparation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(class)
library(e1071) #libreria de naive bayes

source('utils.R')
set.seed(117)
raw_data <- read.csv('dataset.csv')
```

##Data Cleansing
```{r }
raw_data = raw_data[complete.cases(raw_data), ]


```

##Only Numeric Values
```{r }
nums <- unlist(lapply(raw_data,is.numeric))
raw_data2 <- raw_data[,nums]

raw_data2$Churn <- raw_data$Churn


# Particion train - test

h.part <- partition_train_test(raw_data2, ntrain = 28087)
h.train <- h.part$train
h.test <- h.part$test


```

## Naive Bayes
```{r }

h.fit <- naiveBayes(as.factor(h.train$Churn) ~ ., data = h.train)
h.test$yhat <- predict(h.fit, newdata = h.test, type = 'class') 
error <- fn_err_cla(h.test$yhat,h.test$Churn)


#Cross Validation

h.k_folds <- 5
h.cv_part <- partition_cv(h.train, k_folds = h.k_folds)
h.cv_train <- h.cv_part$train
h.cv_test <- h.cv_part$test

h.formulas <- c('as.factor(Churn) ~ .')
cv_err_nv(h.cv_part)


# El error es muy parecido que las priobabilidades a priori. Tanto en test como en cross validation. Por eso naive bayes no es bueno.

```
## KNN

```{r}
library(ElemStatLearn)

#h.k_folds <- 5 Para todos los casos uso k-fold  = 5
#Fitting KNN para predecir el "training set" y predecir los resultados de las pruebas
y_pred = knn(train = h.train[,2:35],h.test[,2:35],h.train$Churn,k = 5)
h.test$ypred = y_pred

#Matriz de confusion
cm = table(h.test$Churn,y_pred) #error_knn = ((cm[2]+cm[3])/(cm[1]+cm[2]+cm[3]+cm[4]))
error_knn = mean(h.test$ypred != h.test$Churn); 


#Probando con diferentes K
ks <- 1:10
error_list_knn <- list()

   for (k in seq(1, 10)) {
     
      y_pred = knn(train = h.train[,2:35],h.test[,2:35],h.train$Churn,k = k)
      h.test$ypred = y_pred
      
      #h.errors_knn$error_knn[k] <- mean(h.test$ypred != h.test$Churn);
      error_list_knn[k] <- mean(h.test$ypred != h.test$Churn);
   }
   
#  Mientras k aumenta disminuye el error, K = 10 es el menor error en este caso
error_list_knn
plot(ks, error_list_knn, type="o",ylab="misclassification error")

#CV
h.k_folds <- 5
h.cv_part <- partition_cv(h.train, k_folds = h.k_folds)
h.cv_train <- h.cv_part$train
h.cv_test <- h.cv_part$test


ks <- 1:10
res <- sapply(ks, function(k) {
  
  res.k <- sapply(seq_along(h.cv_train), function(i) {
     
    pred <- knn(train = h.cv_train[[i]][,2:35],h.cv_test[[i]][,2:35],h.cv_train[[i]]$Churn,k = k)
    mean(pred != h.cv_test[[i]]$Churn);
  })
  ##Promedio para 5 folds
  mean(res.k)
})
res

plot(ks, res, type="o",ylab="cross validation error")

#Basandonos en cross validation k = 9  ofrece el menor error

```

#Logistic Regression 
#Random Forest
#RPART
#Boosting
