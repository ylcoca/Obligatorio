---
title: "Obligatorio"
output: html_document
---

## Preparation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(class)
library(e1071) #libreria de naive bayes

source('utils.R')
set.seed(117)
raw_data <- read.csv('dataset.csv')
```

##Data Cleansing
```{r }
raw_data = raw_data[complete.cases(raw_data), ]


```

##Only Numeric Values
```{r }
nums <- unlist(lapply(raw_data,is.numeric))
raw_data2 <- raw_data[,nums]

raw_data2$Churn <- raw_data$Churn


# Particion train - test

h.part <- partition_train_test(raw_data2, ntrain = 28087)
h.train <- h.part$train
h.test <- h.part$test


```

## Naive Bayes
```{r }

h.fit <- naiveBayes(as.factor(h.train$Churn) ~ ., data = h.train)
h.test$yhat <- predict(h.fit, newdata = h.test, type = 'class') 
error <- fn_err_cla(h.test$yhat,h.test$Churn)


#Cross Validation

h.k_folds <- 5
h.cv_part <- partition_cv(h.train, k_folds = h.k_folds)
h.cv_train <- h.cv_part$train
h.cv_test <- h.cv_part$test

h.formulas <- c('as.factor(Churn) ~ .')
cv_err_nv(h.cv_part)


# El error es muy parecido que las priobabilidades a priori. Tanto en test como en cross validation. Por eso naive bayes no es bueno.

```
## KNN

```{r}
library(ElemStatLearn)

#Fitting KNN to the training set and predicting the test set results
y_pred = knn(train = h.train[,2:35],h.test[,2:35],h.train$Churn,k = 5)
h.test$ypred = y_pred




#making the confusion matrix
cm = table(h.test$Churn,y_pred)
error_knn = ((cm[2]+cm[3])/(cm[1]+cm[2]+cm[3]+cm[4]))



set = h.train
 X1 = seq(min(set[,-36]),max(set[,-36]))
 X2 = seq(min(set[,-36]),max(set[,-36]))
 grid_set = expand.grid(X1,X2)  
 colnames = c('Monthly Revenue','Monthly Minutes')
 y_grid = knn(train = h.train[,-36],grid_set[,-36],h.train$Churn,k = 5)
 plot(set[,-36],
      main = 'K-NN (Trainig Set)',
      xlab = 'Monthly Revenue',ylab = 'Monthly Minutes',
      xlim = range(X1), ylim = range(X2))
 contour(X1,X2, matrix(as.numeric(y_grid),length(X1),length(X2)),add = TRUE)
 points(grid_set,pch = '.',col =ifelse(y_grid == 1,'springgreen3','tomatoe'))
  points(set,pch = 21,bg =ifelse(set[,36] == 'yes','green4','red3'))
#   

#-----------------------
# h.max_complexity <- 50
# h.sims <- 20
# h.A <- function(k, train, test) {
#   knn(train, test, cl = train$Churn, k)
# }
# h.test <- head(h.test, -1)
# 
# h.predTest <- list()
# for (k in seq(h.max_complexity)) {
#   h.predTest[[k]] <- list()
#   for (s in seq(h.sims)) {
#     h.predTest[[k]][[s]] <- h.A(k, h.train[s], h.test)
#    }
# }


#Visualizing test set results
set = h.test

```

#Logistic Regression 
#Random Forest
